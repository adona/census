import csv
import json
import re
import random
from collections import Counter
from statistics import mean, median
from math import floor, ceil, sqrt

verbose = True


def convert_data_dictionary_to_json(filepath_in_IPUMS, rectypes, filepath_out_detailed, filepath_out_compact):
    # Load a data dictionary that was generated by IPUMS (https://www.ipums.org/)
    # and convert it to our internal JSON format.
    # The output will actually be two JSON files:
    #   1.  a "detailed" data dictionary primarily used for loading the data and converting it from 
    #       the "fixed-width text file" format provided by IPUMS to a JSON, and
    #   2.  a "compact" data dictionary primarily used for decoding field values (mapping codes -> descriptions)
    #       during data analysis.

    ### Example use:
    # datadir = "/Users/adona/data/census/timeuse/"
    # filepath_in_IPUMS = datadir + "raw/atus16.cbk"
    # filepath_out_detailed = datadir + "dictionaries/atus16_dictionary_detailed.json"
    # filepath_out_compact = datadir + "dictionaries/atus16_dictionary_compact.json"
    # rectypes = {
    #     "1": "household",
    #     "2": "person",
    #     "3": "activity",
    #     "4": "who",
    #     "5": "eldercare"
    # }
    # convert_data_dictionary_to_json(filepath_in_IPUMS, rectypes, filepath_out_detailed, filepath_out_compact)


    ### Sample input file:  (the IPUMS "basic" version) 

        # Samples selected:
        #       ATUS 2013
        # ... 
        #   Variable               Columns        Len    2013   
        #   RECTYPE            1   1              1      X
        #   SERIAL             1  16-22           7      X
        #   YEAR               1  23-27           5      X 
        #   FAMINCOME          1  49-51           3      X 
        # ...
        # ...
        # FAMINCOME		Family income
        # 001		Less than $5,000
        # 002		$5,000 to $7,499
        # 003		$7,500 to $9,999
        # ... 

    # The structure is: 
    #   - a few lines of header information
    #   - a list of all the fields in the dataset, with field name, what kind of record it applies to
    #       (e.g. for timeuse data: whether it's a household, person, activity, etc field), 
    #       and information on how to load that field from the raw data files
    #   - a list of the categorical fields and each valid entry they can take

    ### Sample output files:

    #### Detailed data dictionary: (used for loading raw data and converting to JSON)

        # {
        #   "household": [
        #     {
        #       "name": "SERIAL",
        #       "rectype": "1",
        #       "len": 7,
        #       "start": 16,
        #       "end": 22,
        #       "field_type": "string"
        #     },
        #     ...
        #     {
        #       "name": "FAMINCOME",
        #       "rectype": "1",
        #       "len": 3,
        #       "start": 49,
        #       "end": 51,
        #       "field_type": "categorical",
        #       "valid_entries": {
        #         "001": "Less than $5,000",
        #         "002": "$5,000 to $7,499",
        #         "003": "$7,500 to $9,999",
        #     ...
        #   ],
        #   "person": [
        #     {
        #     "name": "AGE",
        #     "rectype": "2",
        #     ...
    
    # The structure is a JSON file, containing for each type of record (household, person, etc)
    # the list of applicable fields and their meta-information.

    #### Compact data dictionary: (used for decoding field entries during data analysis)

        # {
        #   "RECTYPE": "string",
        #   "SERIAL": "string",
        #   "FAMINCOME": {
        #     "001": "Less than $5,000",
        #     "002": "$5,000 to $7,499",
        #   ...
    
    # The structure is a dictionary mapping field names to either 
    #   - "string" for non-categorical variables
    #   - the valid entries for categorical variables, represented as a dictionary of {code: description} pairs.

    log("Loading the IPUMS data dictionary.. ")

    data_dictionary = {}
    with open(filepath_in_IPUMS, "r") as f:
        # Find the beginning of the fields list (for now skip the dataset headers)
        while True:
            line = f.readline().strip()
            if(re.match('Variable', line)):
                break

        # Read all the fields
        while True:
            line = f.readline().strip()
            if (line == ""): # Reached the end of the fields list
                break

            # Sample input lines:
                #   RECTYPE            1   1              1      X
                #   SERIAL             1  16-22           7      X
            line = re.split("\s+", line)
            fname = line[0]
            rectype = line[1]
            flen = int(line[3])
            if flen == 1: # If the field length =1, the start and end indices are the same
                fstart = int(line[2])
                fend = fstart
            else:
                frange = line[2].split("-")
                fstart = int(frange[0])
                fend = int(frange[1])

            data_dictionary[fname] = {
                "name": fname,
                "rectype": rectype,
                "len": flen,
                "start": fstart,
                "end": fend,
                "field_type": "string" # Default to this, switch to categorical if I find valid entry descriptions below
            }
        
        # Find the beginning of the categorical fields descriptions
        for i in range(5): # Skip a few lines to get there
            f.readline()

        # Read all the categorical fields descriptions
        # Sample field description:
            # FAMINCOME		Family income
            # 001		Less than $5,000
            # ... 
        while True:
            line = f.readline()
            if (line == ""): # Reached EOF
                break
            line = line.strip()

            # Read the field name
            line = re.split("\t+", line)
            fname = line[0]
            # NOTE: For now ignore the field description since I don't have descriptions
            # for the non-categorical fields

            # Read all the valid entries for this field
            fvalid = {}
            while True:
                line = f.readline().strip()
                if (line == ""): # Done with this field
                    break

                line = re.split("\t+", line)
                if(len(line) == 1):   # This is not a {code: description} pair, but extra annotation I can skip
                    continue
                code = line[0]
                desc = line[1]
                fvalid[code] = desc
            data_dictionary[fname]["field_type"] = "categorical"
            data_dictionary[fname]["valid_entries"] = fvalid
        
    log("IPUMS data dictionary loaded.")


    # Group fields by record type to generate the "detailed" version of the data dictionary.
    # Initialize the detailed data dictionary with empty field lists for each record type.
    data_dictionary_detailed = {}
    for rectype in rectypes.values():
        data_dictionary_detailed[rectype] = []
    # Go through each field and assign it to its corresponding list
    for fname in data_dictionary:
        field = data_dictionary[fname]
        rectype = rectypes[field["rectype"]]
        data_dictionary_detailed[rectype].append(field)
    # Save the detailed dictionary to a JSON file
    log("Saving the detailed version of the data dictionary..")
    with open(filepath_out_detailed, "w") as f:
        f.write(json.dumps(data_dictionary_detailed, indent=2))
    
    # Finally, generate and save the "compact" version of the data dictionary.
    data_dictionary_compact = {}
    for fname in data_dictionary:
        field = data_dictionary[fname]
        ftype = field["field_type"]
        if ftype == "categorical":
            data_dictionary_compact[fname] = field["valid_entries"]
        else: 
            data_dictionary_compact[fname] = ftype
    log("Saving the compact version of the data dictionary..")
    with open(filepath_out_compact, "w") as f:
        f.write(json.dumps(data_dictionary_compact, indent=2))

    log("Data dictionary preprocessing complete!")

def parse_fixedwidth_datafile_line(line, data_dictionary_detailed, rectypes):
    # Takes a line of a datafile encoded as a "fixed width text file",
    # and a data dictionary, and returns the parsed line: 

    # Example raw text line:
        # 2201301011300040201301001010011899905.66203400022020202060501 ...
        #                 |person #|  ...               |age|sex| ...

    # Example data dictionary entry:
        # {
        #   "name": "AGE",
        #   "rectype": "2",
        #   "len": 3,
        #   "start": 47,
        #   "end": 49,
        #   "field_type": "string"
        # },

    record = {}
    rectype = rectypes[line[0]]
    for field in data_dictionary_detailed[rectype]:
        record[field["name"]] = line[(field["start"]-1):field["end"]]
    return record

### Compute basic statistics directly over the weighted dataset

def weighted_len(data, weights_field):
    return sum([d[weights_field] for d in data])

def weighted_mean(data, field, weights_field):
    if len(data) == 0:
        return None
    else: 
        w_mean = 0
        for d in data:
            w_mean += float(d[field])*d[weights_field]
        w_len = weighted_len(data, weights_field)
        w_mean = w_mean / w_len
        return w_mean

def weighted_median(data, field, weights_field):
    if len(data) == 0:
        return None
    else: 
        # Sort the data
        sorted_data = sorted(data, key=lambda row: row[field])

        # Calculate what the middle index of the expanded dataset would be
        w_count = weighted_len(sorted_data, weights_field)
        w_middle_idx = (w_count+1) // 2

        # Start walking the unexpanded (weighted) dataset, adding up weights to calculate 
        # what the index in the expanded dataset would be, until I reach w_middle_index
        w_idx = 0
        w_median = None
        for d in sorted_data:
            w_idx += d[weights_field]
            if(w_idx >= w_middle_idx): # Found it!
                w_median = d[field]
                break

        return w_median

def weighted_counter(data, field, weights_field):
    # Initialize counter to all zeros
    field_values = list(set([d[field] for d in data]))
    w_counter = {v: 0 for v in field_values}
    # Compute the weighted counts
    for row in data:
        w_counter[row[field]] += row[weights_field]
    # Convert to list of dictionaries
    w_counter_list = []
    for key in w_counter:
        w_counter_list.append({
            "key": key,
            "count": w_counter[key]
        })
    # Sort descending by count
    w_counter_list = sorted(w_counter_list, key=lambda x: x["count"], reverse = True)
    # Annotate with percentage of total count and cumulative distribution
    total_count = weighted_len(data, weights_field)
    cumulative = 0
    for entry in w_counter_list:
        entry["perc"] = entry["count"]/total_count*100 
        cumulative += entry["perc"]
        entry["cumulative"] = cumulative
    return w_counter_list

def compute_estimate_and_standard_error(f, weight_field, replicate_fields):
    # All census datasets use a method based on replicate weights to compute 
    # standard error estimates. See explanation here: https://cps.ipums.org/cps/repwt.shtml

    # Compute the estimate using the default weight
    estimate = f(weight_field)
    # Also re-compute the estimate using each replicate weight
    # and use them to compute the standard error
    se = 0
    for i in range(160):
        rw = replicate_fields+str(i+1)
        replicate_estimate = f(rw)
        se += (estimate - replicate_estimate) ** 2
    se = sqrt(4/160*se)
    return estimate, se

### Generate unweighted dataset by expanding and subsampling the data

def expand_and_subsample_data(data, weights_field, subsampling_factor, randseed = None):
    if subsampling_factor < 1:
        log("Expanding and subsampling the dataset with a subsampling factor = " + str(subsampling_factor) + " ...")
    else: 
        log("Expanding dataset... ")

    es_data = []
    random.seed(randseed)
    nrows = 0
    for row in data:
        weight = row[weights_field] # = number of copies of this datapoint in the expanded dataset
        for i in range(weight): # for each copy
            if (random.random() < subsampling_factor): # subsample
                es_data.append(row)
        nrows += 1
        if (nrows % 10000 == 0):
            log("Record #: " + str(nrows))
            
    log("Expanding and subsampling complete.")    
    log("Initial # records: " + str(len(data)))
    log("Final # records: " + str(len(es_data)))
    return es_data

### Misc helper functions

def load_csv_data(filepath, fields = "All"):
    log("Loading data from: " + filepath + "..")
    log("Loading fields: " + str(fields))
            
    with open(filepath, "r") as f:
        r = csv.DictReader(f)
        data = []
        nrows = 0
        for row in r:
            if(fields == 'All'):
                data.append(row)
            else:
                data.append({field: row[field] for field in fields})
            nrows += 1
            if (nrows % 10000 == 0):
                log("Record #: " + str(nrows))
        log("Finished loading file: " + filepath)
        log("# of records: " + str(len(data)))
    
    return data

def save_data_to_csv(data, filepath, fields = "All"):
    log("Saving data to: "+ filepath + "..")
    if fields == "All":
        fields = list(data[0].keys())
    log("Saving fields: " + str(fields))
    log("# of records to write: " + str(len(data)))

    with open(filepath, "w") as f:
        writer = csv.DictWriter(f, fieldnames=fields)
        writer.writeheader()
        nrows = 0
        for row in data:
            if (fields == "All"):
                writer.writerow(row)
            else: 
                writer.writerow({field: row[field] for field in fields})
            nrows += 1
            if (nrows % 10000 == 0):
                log("Record #: " + str(nrows))

    log("Data save finished.")

def load_JSON(filepath):
    log("Loading: " + filepath + "..")
    with open(filepath, "r") as f:
        data = json.loads(f.read())
    return data

def save_JSON(data, filepath):
    log("Saving: " + filepath + "..")
    with open(filepath, "w") as f:
        f.write(json.dumps(data, indent=2))

def parse_dollar_amt(amt_string):
    # Takes a string like "$12,500", strips the "$" and ",", and converts to int
    return int(amt_string[1:].replace(",",""))

def log(text):
    if verbose:
        print(text)
